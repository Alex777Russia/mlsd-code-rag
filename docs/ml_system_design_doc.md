# ML System Design Doc - [RU]
## Дизайн ML системы - Code-RAG MVP 1.0
*Шаблон ML System Design Doc от телеграм-канала [Reliable ML](https://t.me/reliable_ml)*

### 1. Цели и предпосылки
#### 1.1. Зачем идем в разработку продукта?
- **Бизнес-цель** `Product Owner`:
  Создать open-source фреймворк, который упрощает и ускоряет процесс понимания незнакомых кодовых баз для разработчиков. Цель — сократить время "onboarding" в новый проект и повысить продуктивность за счет автоматизированного ассистента, способного отвечать на вопросы по коду. В рамках учебного курса — продемонстрировать навыки проектирования ML-систем.

- **Почему станет лучше, чем сейчас, от использования ML** `Product Owner` & `Data Scientist`:
  Сейчас разработчики тратят часы и дни на ручное изучение кода, чтение документации (если она есть) и общение с коллегами. ML-подход (RAG) позволяет создать "эксперта" по любой кодовой базе за минуты. Это централизует знания, делает их доступными 24/7 и позволяет получать ответы, контекстуально связанные с конкретными частями кода, что невозможно при использовании глобального поиска.

- **Что будем считать успехом итерации с точки зрения бизнеса** `Product Owner`:
  Успехом MVP будет считаться работающий прототип, который:
  1.  Успешно обрабатывает публичный GitHub репозиторий средних размеров (например, сам этот проект или небольшой фреймворк).
  2.  Дает релевантные ответы на 5-10 тестовых вопросов о структуре кода и назначении ключевых функций.
  3.  Публикация проекта на GitHub и получение первых "звезд" и обратной связи от сообщества.

#### 1.2. Бизнес-требования и ограничения
- **Краткое описание БТ и ссылки на детальные документы с бизнес-требованиями** `Product Owner`:
  Сервис должен предоставлять UI, куда пользователь вводит URL публичного GitHub репозитория. Система скачивает репозиторий, индексирует его и предоставляет чат-интерфейс для взаимодействия.
  Требования:
  - **Индексация:** поддержка `.py, .md, .cpp, .java, .go, .js`.
  - **Взаимодействие:**
    1.  Ответы на вопросы о коде на естественном языке.
    2.  Генерация кода (дополнение, рефакторинг, тесты).
  - **Архитектура:** должна быть модульной для добавления различных парсеров, моделей ранжирования и других модулей улучшения качества поиска.
  - **LLM:** поддержка любой модели через OpenAI-совместимый API.

- **Бизнес-ограничения** `Product Owner`:
  1.  **Ресурсы:** Проект разрабатывается в рамках учебного курса с ограниченным бюджетом. Предпочтение отдается open-source моделям и локальным/бесплатным векторным базам данных (ChromaDB, FAISS).
  2.  **Время:** MVP должен быть готов в рамках учебного курса.
  3.  **Безопасность:** На этапе MVP обрабатываются только публичные репозитории, вопросы приватности и безопасности кода не являются приоритетом.

- **Что мы ожидаем от конкретной итерации** `Product Owner`:
  Рабочий Streamlit/Gradio интерфейс, демонстрирующий end-to-end пайплайн: URL -> Индексация -> Ответ на вопрос.

- **Описание бизнес-процесса пилота, насколько это возможно** `Product Owner`:
  "Пилотом" будет являться демонстрация работы сервиса на известном open-source репозитории (например, `FastAPI` или `Requests`). Мы зададим заранее подготовленный список вопросов, ответы на которые можно верифицировать вручную, и оценим качество и релевантность ответов.

- **Что считаем успешным пилотом?** `Product Owner`:
  Успешный пилот — это когда система правильно отвечает как минимум на 70% общих вопросов ("За что отвечает файл `main.py`?") и на 50% специфических вопросов ("Какая функция в этом репозитории выполняет аутентификацию пользователя?").

#### 1.3. Что входит в скоуп проекта/итерации, что не входит
- **На закрытие каких БТ подписываемся в данной итерации** `Data Scientist`:
  - **Входит в MVP:**
    - Клонирование репозитория по URL.
    - Парсинг и чанкинг файлов с кодом (`.py`, `.md` в первую очередь).
    - Векторизация и сохранение в локальную векторную БД (ChromaDB).
    - Базовый RAG-пайплайн: `retrieve` -> `generate`.
    - Простейший UI для ввода URL и вопросов.
    - Поддержка подключения к LLM через OpenAI API.

- **Что не будет закрыто** `Data Scientist`:
  - **Не входит в MVP:**
    - Продвинутое обогощение контекстов метаданными: добавление Graph-RAG, суммаризация контекстов.
    - Продвинутые модули: `reranker`, `rag_fusion`, и другие компоненты улучшения поиска.
    - Сложный UI с историей чатов, управлением пользователями.
    - Поддержка приватных репозиториев и аутентификация.
    - Оптимизация под высокую нагрузку и масштабирование.
    - Инкрементальная индексация (обновление индекса при изменениях в репозитории).
    - Fine-tuning кастомных/собственных моделей.

- **Описание результата с точки зрения качества кода и воспроизводимости решения** `Data Scientist`:
  Код будет соответствовать стандартам `ruff`, покрыт pre-commit хуками. Вся настройка будет описана в `README.md`, а зависимости зафиксированы в `requirements.txt`, что обеспечит воспроизводимость.

- **Описание планируемого технического долга** `Data Scientist`:
  - Отсутствие асинхронной обработки: индексация будет блокирующим процессом.
  - Упрощенный чанкинг: на первом этапе будет использоваться рекурсивный сплиттер по символам, а не language-specific парсеры (например, `tree-sitter`).
  - Обогощение контекстов метаинформацией: при индексации репозитория каждый чанк будет иметь примитивную метаинформаицию (название файла, путь файла, родительский класс, название функции)
  - Хранение данных: использование локальной БД без персистентности между запусками.

#### 1.4. Предпосылки решения
- **Описание всех общих предпосылок решения** `Data Scientist`:
  1.  **Гипотеза RAG:** Мы предполагаем, что семантический поиск по чанкам кода с последующей генерацией ответа LLM является эффективным методом для ответов на вопросы о кодовой базе.
  2.  **Качество эмбеддингов:** Мы исходим из того, что существующие модели эмбеддингов (например, `jina-embeddings-v2-base-code` или другие open-source аналоги) способны улавливать семантику программного кода.
  3.  **Доступ к LLM:** Предполагается наличие доступа к LLM (API, vLLM).
  4.  **Структура кода:** Мы предполагаем, что код в репозиториях структурирован достаточно хорошо, чтобы разделение на функции/классы было осмысленным.

### 2. Методология `Data Scientist`
#### 2.1. Постановка задачи
- **Что делаем с технической точки зрения** `Data Scientist`:
  Мы строим систему **Retrieval-Augmented Generation (RAG)**. Задача состоит из двух основных и одного дополнительного этапов:
  1.  **Offline-индексация:** Создание и наполнение векторной базы знаний на основе исходного кода репозитория.
  2.  **Online-генерация:** Поиск релевантных фрагментов кода (контекста) в базе по запросу пользователя и генерация ответа с помощью LLM на основе этого контекста.
  3.  **Агентский сценарий:** Расширенный поиск релевантных контекстов, когда даём возможность агенту использовать поиск и ряд других дополнительных тулов для формирования более качественного ответа.

#### 2.2. Блок-схема решения

Code-RAG состоит из 3-х компонент:
  - Data Enrichment
  - Search Engine
  - Agentic

**2.2.1. Пайплайн индексации (Data Enrichment):**

Пайплайном индексации репозитория управляет класс `Data Enrichment`. Весь pipeline состоит из последовательного взаимодействия нескольких модулей: `LoaderConnector` - скачивает сам репозиторий, `Parser` - обходит репозиторий и обрабатывает нужные файлы из него, нарезая на чанки, и `Vectorizer` - векторизует с помощью модели эмбеддера чанки. Результат работы индексатора сохраняется в векторную базу `Qdrant`.

![Data Enrichment](assets/images/workflow_DataEnrichment.png)

**2.2.2. Пайплайн поиска (Search Engine):**

Пайплайном поиска упарвляет класс `Search Engine`, который состоит из нескольких последовательно идущих модулей, сердцем является модуль `Retriever`. Модуль способен обрабатывать как одиночный запросы пользователя, так и поддерживает обработку истории диалога с пользователем с помощью модуля `QueryRewriter`.

![Search Engine](assets/images/workflow_SearchEngine.png)

**2.2.3. Пайплайн агентского сценария (Agentic):**

Агентский сценарий позволяет декомпозировать запрос пользователя, настраивать конфигурацию поиска, ходить в открытое API для получения актуальной информации по необходимым пакетам и библиотекам, а также агрегировать и суммаризировать итоговый результат каждого модуля. Так же агент способен оценивать качество выполнения всего процесса и, при необходимости, повторять запросы с другими параметрами, пока не будет получен удовлетворяющий агента ответ или не закончатся лимит на итерации.

![Agentic scenario](assets/images/workflow_Agentic.png)


#### 2.3. Этапы решения задачи `Data Scientist`

#### Этап 1. Подготовка данных

- Описание данных/сущностей находится в разделе: "2.3.1. Описание данных"
- Описание процесса генерации данных находится в разделе: "2.3.2. Процесс генерации данных"
- Данных достаточно. Однако при необходимости можно расширить (весь GitHub > более 420 млн.) и нагенерировать синтетических вопросов (через LLM / в ручную).
- Конфиденциальной информации в данных нет - они полностью публичные.
- На данном этапе необходимо получить предобработанный датасет (репозитории, порезанные на чанки, а так же вопросы к ним).


#### 2.3.1. Описание данных

В качестве источника данных был выбран публичный бенчмарк [SWE-QA-Bench](https://github.com/peng-weihan/SWE-QA-Bench/). В него включены 15 популярных Python-репозиториев, охватывающих различные типы проектов:

<details>
  <summary>Список репозиториев:</summary>

        - Astropy
        - Django
        - Flask
        - Matplotlib
        - Pylint
        - Pytest
        - Requests
        - Scikit-learn
        - Sphinx
        - Sqlfluff
        - Sumpy
        - Xarray
        - Conan
        - Reflex
        - Steamlink
</details>


#### 2.3.2. Процесс генерации данных

Для каждого репозитория была выполнена загрузка исходного кода и автоматическая обработка. Парсер обходит все файлы репозитория, исключает файлы и директории согласно конфигурации (параметр `exclude`), и обрабатывает только поддерживаемые типы файлов (параметр `ast_chunker_languages`).

Все файлы с кодом разбиваются с использованием AST-чанкинга: каждая функция или класс выделяются в отдельный чанк.
Для остальных файлов применяется нарезка по строкам (100 строк с перекрытием 20).
Каждый чанк содержит сырой текст и метаданные: путь к файлу, имя файла, диапазон строк, язык, размер в количестве символов и уникальный идентификатор.

<details>
  <summary>Пример чанка:</summary>

    [
      {
        "content": "def pytest_configure(config):\n    PYTEST_HEADER_MODULES[\"PyERFA\"] = \"erfa\"\n    PYTEST_HEADER_MODULES[\"Cython\"] = \"cython\"\n    PYTEST_HEADER_MODULES[\"Scikit-image\"] = \"skimage\"\n    PYTEST_HEADER_MODULES[\"pyarrow\"] = \"pyarrow\"\n    PYTEST_HEADER_MODULES[\"asdf-astropy\"] = \"asdf_astropy\"\n    TESTED_VERSIONS[\"Astropy\"] = __version__",
        "metadata": {
            "chunk_id": "b46ec686-da54-4613-8bb1-496dd64ae6b7",
            "filepath": "conftest.py",
            "file_name": "conftest.py",
            "chunk_size": 328,
            "line_count": 7,
            "start_line_no": 22,
            "end_line_no": 28,
            "node_count": 46,
            "language": "python"
        }
      },
      ...
    ]

</details>

Все репозитории были проиндексированы таким образом и собраны в единый датасет, который имеет таку структуру:
```
SWE-QA-Bench-Dataset/
├── chunks/
│   ├── astropy.json
│   ├── django.json
│   ├── ...
├── questions/
│   ├── astropy.jsonl
│   ├── ...
├── repos/
    ├── astropy/
        ├── ...
    ├── ...
```

Данные структурированы и сохранены на [Яндекс Диск](https://disk.yandex.ru/d/hcGLPoNdE8l9hQ).

#### 2.3.3. Анализ данных

Статистика:
- всего: **80,710** чанков и **705** вопросов (по **47** вопросов на каждый репозиторий)
- менее **0,05%** всех строк - это комментарии и документация
- больше всего чанков относятся к типу **test**, **src** и **scripts**, затем **docs** и **config**
- более 95% всех чанков относяткся к **Python**, однако так же имеется **JavaScript** и **C++**

<details>
  <summary>Графики и диаграммы:</summary>

  Distribution of collected issues | Distribution of question types
  :-------------------------:|:-------------------------:
  ![Distribution of collected issues by 11 repos](assets/images/distribution_of_collected_issues.png)  |  ![Distribution of question types by 11 repos](assets/images/distribution_of_question_types.png)

  ![Token count distribution](assets/images/tokens_count_distribution.png)

  ![Files types composition by repo](assets/images/files_types_composition_by_repo.png)

  ![Language composition by repo](assets/images/language_composition_by_repo.png)

  ![Feature correlations](assets/images/feature_correlations.png)
</details>

Выводы:
- Малое покрыте комментариями показывает, что в чанках преимущественно содержится код, а следовательно при векторизации надо будет использовать задачу `nl2code.passage` (из jina-api, например). Для векторизации пользовательского запроса необходимо будет использовать `nl2code.query`.
- В среднем на каждый репоиторий приходится ~ 5,380 чанков, а следовательно важно подобрать наиболее точную конфигурацию, т.к. при передаче даже 100 чанков в контекст LLM скажется на качестве генеративного ответа.
- Около 80% всех чанков имеет кол-во токенов, лежащих в диапазоне от 10 до 100, следовательно может быть полезным использовать методы расширения контекста (на 1-2 соседних чанка) для найденных источников, чтобы контекст был цельным.


#### Этап 2. Подготовка прогнозных моделей

Поскольку мы строим RAG-систему, этап "моделирования" здесь трансформируется в этап построения пайплайна Retrieval (поиска) и Generation (генерации). Классическое обучение с функцией потерь (Loss) на старте отсутствует (используются предобученные модели), но метрики оценки качества критически важны.

Проблема данных: Датасет `SWE-QA-Bench` содержит пары (`question`, `answer`), но не содержит прямой ссылки на конкретный `chunk_id` (Ground Truth context), в котором содержится ответ.

Решение: Мы будем использовать гибридную стратегию валидации — синтетическую генерацию для оценки Retriever'а и подход LLM-as-a-Judge (RAGAS) для оценки всего пайплайна.

#### 2.4.1. ML-метрики и функции потерь
Для RAG системы метрики делятся на две группы: качество поиска (Retrieval) и качество ответа (Generation).

1. Метрики поиска (Retrieval Metrics):
Для оценки этих метрик мы сгенерируем синтетический датасет "Chunk -> Question" (см. Схему валидации).

    - Hit Rate @ k (k=5, 10): Доля запросов, для которых правильный чанк оказался в топ-k выдаче.
    Обоснование: Пользователю важно, чтобы LLM вообще увидела правильный кусок кода. Если контекст не попал в топ, ответ будет галлюцинацией.

    - MRR @ k (Mean Reciprocal Rank): Показывает, насколько высоко в выдаче находится релевантный документ.
    Обоснование: LLM лучше работает, если релевантная информация находится в начале контекста ("Lost in the middle" problem).

2. Метрики генерации (RAGAS / LLM-as-a-Judge):
Оцениваются с помощью (GPT-4o/любой другой более умной LLM) на реальных вопросах из SWE-QA-Bench.

    - Context Recall: Оценивает, насколько найденный контекст соответствует эталонному ответу (Ground Truth Answer).
    Обоснование: Так как у нас нет разметки чанков, мы просим судью оценить: "Содержится ли информация, необходимая для этого ответа, в найденных фрагментах?".

    - Faithfulness (Верность контексту): Оценка того, не выдумала ли модель ответ, которого нет в найденном коде.
    Обоснование: В задачах по коду галлюцинации (выдуманные аргументы функций, несуществующие методы) недопустимы.

    - Answer Correctness: Семантическая близость сгенерированного ответа и эталонного ответа (из датасета).

Функция потерь (Loss Function):
На этапе MVP обучение не производится.
Стратегия на будущее (Fine-tuning): Если качество эмбеддингов будет низким, планируется дообучение Bi-Encoder с использованием Multiple Negatives Ranking Loss (MNRL) или Contrastive Loss. Это позволит "сблизить" вектора вопросов и соответствующих кусков кода.


#### 2.4.2. Схема ML-валидации
Учитывая отсутствие `context_id` в разметке, валидация проходит в два потока.

Поток А: Синтетическая оценка `Retriever` (Technical Validation)

1. Генерация: Берем случайные 5% чанков из каждого репозитория. Используем LLM (например, gpt-4o) для генерации вопроса к каждому чанку: "Сформулируй вопрос, ответом на который служит этот код".
2. Dataset: Получаем пары (Generated_Question, Chunk_ID).
3. Оценка: Прогоняем вопросы через поиск и считаем Hit Rate@5.
    - Цель: Техническая проверка, что эмбеддер вообще понимает семантику кода (nl2code).

Поток Б: `E2E` оценка на SWE-QA-Bench (Business Validation)

1. Hold-out: Откладываем 20% вопросов (по ~10 вопросов на репо) для финального теста. Остальные используем для подбора гиперпараметров (chunk size, top_k).
2. Process:
    - Берём вопрос.
    - Ищем Top-K чанков.
    - Генератор даёт ответ.
3. Scoring: Используем фреймворк RAGAS. Скармливаем тройку (`Question`, `Answer`, `Context`) и `Ground_Truth_Answer` модели-судье (GPT-4o).
    - Судья выставляет баллы за Correctness и Context Recall.

Нефункциональные требования:
  - Latency: Замеряем время от запроса до начала стриминга токенов (TTFT) и общее время. Цель < 5 сек.

#### 2.4.3. Структура бейзлайна (Baseline MVP)
В качестве отправной точки (Baseline) выбираем конфигурацию без сложных эвристик.

- Предобработка:
  - Использование чанков из этапа 1 (AST-split).
  - Добавление метаданных в текст для векторизации: `Text for embedding = "File: {filepath} \n Code: {content}"`. Это помогает различать одинаковые функции (например, main) в разных файлах.
- Embedder (Модель): jina-embeddings-v2-base-code.
  - *Почему*: Специализированная модель для кода, поддерживает контекст 8192 токенов (важно для длинных файлов), SOTA среди open-source моделей небольшого размера.
  - Vector DB: Qdrant (in-memory / local disk).
  - Retriever: Dense Retrieval (поиск по косинусному расстоянию). Top-K = 5.
  - Generator: gpt-4o-mini (через API).
    - *System Prompt*: "Ты — эксперт по анализу кода. Используй ТОЛЬКО предоставленный контекст. Если ответа нет в контексте, скажи об этом. Не выдумывай код."
- Пост-процессинг: Отсутствует в бейзлайне.

#### 2.4.4. Стратегии дальнейшего развития (Advanced)
Если бейзлайн не достигнет целевых метрик (HitRate > 70%), применяем следующие улучшения (в порядке приоритета):

1. Hybrid Search (BM25 + Dense):
    - Добавить поиск по ключевым словам (BM25). Это критически важно для поиска точных совпадений имен переменных, классов или специфических ошибок, где семантика может размываться.
2. Reranking (Cross-Encoder):
    - Извлекать Top-50 кандидатов и переранжировать их моделью qwen/qwen3-reranker-0.6b. Это повысит точность топ-5, подаваемых в LLM.
3. Query Expansion / HyDE:
    - Использовать LLM для генерации "гипотетического кода" по вопросу пользователя, и искать вектора, близкие к этому коду, а не к тексту вопроса.
4. Meta-data Filtering:
    - В UI дать возможность пользователю фильтровать поиск по папкам или расширениям файлов (например, "искать только в /src").

#### 2.4.5. Анализ и интерпретация работы модели
Для понимания качества работы эмбеддингов на этапе MVP будет проведен визуальный анализ:
1. UMAP-проекция: Визуализация векторов чанков одного репозитория (например, Flask).
    - Ожидание: Чанки из одной директории или одного класса должны кластеризоваться вместе. Тесты (/tests) должны быть отделены от логики приложения.
2. Retrieval Analysis: Ручной просмотр топ-3 результатов для 20 случайных вопросов.
    - Анализ причин ошибок: "Нашел похожий код, но не тот файл" vs "Вообще не понял семантику".

#### 2.4.6. Риски этапа и способы снижения
| Риск | Вероятность | Последствия | Способ снижения |
| --- | --- | --- | --- |
| "Poisoned Context" | Высокая | В топ-5 попадает нерелевантный код, сбивающий LLM с толку. | Использовать жесткий промпт "Игнорируй нерелевантное" и добавить Reranker. |
| Code Ambiguity | Средняя | Модель путает функции с одинаковыми именами (напр., setup или init). | Встраивать полный путь файла в вектор (уже учтено в Baseline). |
| Низкое качество оценки | Средняя | RAGAS/LLM-судья ошибается в оценке специфичного кода. | Ручная валидация выборки из 50 ответов для калибровки "судьи". |
| Превышение контекста | Низкая | Промпт с 5 чанками не влезает в LLM. | Использовать модели с 128k контекстом или динамически уменьшать  Top-K. |

#### 2.4.7. Необходимый результат этапа
1. Реализованный класс VectorDBClient (на базе `Qdrant`).
2. Скрипт валидации evaluate_rag.py, который считает HitRate (на синтетике) и RAGAS Scores (на SWE-QA-Bench).
3. Зафиксированный Baseline (код + конфиг), показавший метрики (например, HitRate@5 > 0.4 на синтетике).
4. Отчет (в MLFlow или Markdown) с результатами экспериментов Baseline.

#### 2.4.8. Полезная информация
- В кодовых базах часто встречается дублирование. Важно при индексации использовать дедупликацию чанков (по хэшу контента), чтобы не засорять топ выдачи одинаковыми кусками.
- Для C++ и Python эмбеддинги могут вести себя по-разному. Важно проверить метрики в разрезе языков программирования.

####  Этап 3. Интерпретация и отладка RAG-пайплайна (System Observability)
В RAG-системах интерпретация означает прозрачность поиска: пользователь должен понимать, почему модель дала такой ответ и на какие файлы она смотрела.

- Механизм атрибуции источников (Citations):
  - Реализация функционала, который вместе с ответом возвращает список использованных чанков (имя файла, номера строк).
  - В UI это должно отображаться как "Использованные источники" с возможностью развернуть код. Это критически важно для доверия разработчика к ответу ("Trust, but verify").
- Визуализация процесса поиска (Debug Mode):
  - Для разработчиков системы (вас) добавить режим отладки, выводящий:
    - Скор релевантности (similarity score) топ-5 чанков.
    - Исходный текст чанков до передачи в LLM.
    - Системный промпт, сформированный для LLM.
- Анализ "отказов" (Fallback analysis):
  - Настройка логирования случаев, когда Retriever нашел чанки с низким скором (ниже порога, например < 0.7) и система ответила "Я не нашла информации в репозитории". Это поможет отличить отсутствие знаний от плохого поиска.


#### Этап 4. Интеграция бизнес-правил и Guardrails
ML-модель генерирует текст, но продукт должен работать предсказуемо и безопасно.

- Управление контекстным окном (Cost & Error Management):
  - Правило: Если суммарный объем найденных топ-5 чанков превышает лимит контекста модели (например, 128k токенов) или бюджет пользователя, применять стратегию "Trim" (обрезание) или уменьшения top_k.
  - Обоснование: Избежать ошибок API context_length_exceeded и перерасхода средств на OpenAI.
- Фильтрация выдачи (Output Formatting):
  - Правило: Принудительное форматирование блоков кода в Markdown (python ... ) на уровне системного промпта и пост-процессинга.
  - Правило: Если вопрос не касается программирования (например, "Как приготовить пиццу?"), система должна отвечать заглушкой: "Я могу отвечать только на вопросы по коду данного репозитория".
- Rate Limiting:
  - Ограничение количества запросов от одного пользователя в минуту (защита от DDOS и слива бюджета API ключа).

##### Этап 5. Подготовка инференса и UI (Deployment)
Превращение Python-скриптов в работающее веб-приложение.

- Архитектура сервиса:
  - Backend: FastAPI (или встроенный сервер Streamlit) для обработки запросов.
  - Frontend: Streamlit для быстрого прототипирования (MVP). Интерфейс должен содержать поле ввода URL репозитория, кнопку "Индексировать" (с прогресс-баром) и чат-окно.
  - Vector DB: Запуск Qdrant в Docker-контейнере или использование QdrantClient(":memory:") для упрощения деплоя MVP.
- Контейнеризация:
  - Подготовка Dockerfile и docker-compose.yml, поднимающего UI и базу данных одной командой.
- Кэширование:
  - Реализация кэширования эмбеддингов и результатов индексации (чтобы не перекачивать репозиторий, если пользователь обновил страницу). Использование diskcache или локального сохранения Qdrant.
